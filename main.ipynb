{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 115/150 [02:32<00:49,  1.40s/it]"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#model and \n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from models import MSE,MDcR,DSE,MVBG,CPCA,DPCA,MVP,LPP,LE\n",
    "import scipy\n",
    "datasets_names = ['BBC','MSRC-v1','NGs','Reuters','YALE']\n",
    "datasets_names = ['YALE']\n",
    "\n",
    "for dt_name in datasets_names:\n",
    "    # data\n",
    "    X,labels=read_data(f'datasets/data sets/{dt_name}.mat')\n",
    "    X = [X[0,i].T for  i  in range(X.shape[1])]\n",
    "    X =  [x[:300,] for x in X] #d<=300 ,n = X[0].shape[1]\n",
    "    if isinstance(X[0],scipy.sparse._csr.csr_matrix):\n",
    "        X =   [np.asarray(x.todense()) for x in X]\n",
    "    labels  = labels.squeeze()\n",
    "    k = len(set(labels))\n",
    "    #models\n",
    "    model_list =  [MSE(), MDcR(),DSE(),MVBG(0.1,2,0.1), CPCA(),DPCA(), MVP(),LPP(),LE()]\n",
    "    #main \n",
    "    res_nmi = [];res_acc =[];res_ari=[];res_purity  = []\n",
    "    d_max  =  min(X[0].shape[1],int(np.min([x.shape[0] for x in X]))  )\n",
    "    d_range  = range(k,d_max)\n",
    "    model  =  LE()\n",
    "\n",
    "    for d_ in tqdm(d_range): \n",
    "        nmi_list = [];acc_list =[]; ari_list=[]; purity_list=[]\n",
    "        for i in range(1):\n",
    "            # train_test split\n",
    "            train_idx,test_idx = random_index(X[0].shape[1],0.1)\n",
    "            X_train = [x[:,train_idx] for x in X]\n",
    "            X_test = [x[:,test_idx] for x in X]\n",
    "            y_test = labels[test_idx]\n",
    "            #train\n",
    "\n",
    "            # pred =  model.predict(X_test,0.5,2,1e6,d_,k,10) #MVP\n",
    "            # pred   = model.predict(X_train,X_test,1e7,d_,k,200) #lPP ok too slow\n",
    "            pred  = model.predict(X_test,d_,20,k) #LE ok\n",
    "            # pred = model.predict(X_train,X_test,d_,k) #DPCA ok\n",
    "\n",
    "            # criterion\n",
    "            #nmi\n",
    "            from sklearn.metrics.cluster import normalized_mutual_info_score,adjusted_rand_score\n",
    "            from sklearn.metrics import homogeneity_score\n",
    "\n",
    "            nmi = normalized_mutual_info_score(y_test,pred)\n",
    "            nmi_list.append(nmi)\n",
    "            #acc\n",
    "            acc =   acc_score(y_test,pred)\n",
    "            acc_list.append(acc)\n",
    "            #ari\n",
    "            ari =   adjusted_rand_score(y_test,pred)\n",
    "            ari_list.append(ari)\n",
    "            #purity\n",
    "            purity  =  purity_score(y_test,pred)\n",
    "            purity_list.append(purity)\n",
    "        #\n",
    "        nmi = np.mean(nmi_list)\n",
    "        acc  = np.mean(acc_list)\n",
    "        ari = np.mean(ari_list)\n",
    "        purity = np.mean(purity_list)\n",
    "        # \n",
    "        res_nmi.append(nmi)\n",
    "        res_acc.append(acc)\n",
    "        res_ari.append(ari)\n",
    "        res_purity.append(purity)\n",
    "    np.save(f'./result/nmi/{dt_name}/{model.name}.npy',res_nmi)\n",
    "    np.save(f'./result/acc/{dt_name}/{model.name}.npy',res_acc)\n",
    "    np.save(f'./result/ari/{dt_name}/{model.name}.npy',res_ari)\n",
    "    np.save(f'./result/purity/{dt_name}/{model.name}.npy',res_purity)\n",
    "\n",
    "    #plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.plot(d_range,res_nmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, (300, 149))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "d_,X_test[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics import accuracy_score,homogeneity_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from utils import *\n",
    "a = np.array([1,1,2,2,3,3])\n",
    "b = np.array([3,3,1,2,3,1])\n",
    "# b  = np.array([1,1,3,2,1,3])\n",
    "\n",
    "adjusted_rand_score(a,b)\n",
    "# purity_score(b,b)\n",
    "purity_score(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "X,labels=read_data(f'datasets/data sets/Reuters.mat')\n",
    "X = [X[0,i].T for  i  in range(X.shape[1])]\n",
    "np.asarray(X[2].todense()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./result/NGs/mvbg_nmi.npy',res_nim)\n",
    "res_nmi = np.load('./result/nmi/YALE/nmi_DPCA.npy')\n",
    "res_nmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MVBG\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#model and \n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from models import MSE,MDcR,DSE,MVBG,PCA\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "X,labels=read_data('datasets/data sets/NGs.mat')\n",
    "X = [X[0,i].T for  i  in range(X.shape[1])]\n",
    "labels  = labels.squeeze()\n",
    "\n",
    "d_range = (10,12)\n",
    "model_mvbg = MVBG(0.1,2,0.1)\n",
    "res_nmi = []\n",
    "for d_ in d_range:\n",
    "    nmi_list=[]\n",
    "    for _ in range(1):\n",
    "        dim_emb = model_mvbg.mvbg(X,100,d_,10)\n",
    "        pred =kmeans(dim_emb,7)\n",
    "        nmi_temp = normalized_mutual_info_score(labels, pred)\n",
    "        nmi_list.append(nmi_temp)\n",
    "    nmi = np.mean(nmi_list)\n",
    "    res_nmi.append(nmi)\n",
    "print(res_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSE 單獨\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "X,labels=read_data('datasets/data sets/YALE.mat')\n",
    "X = [X[0,i].T for  i  in range(X.shape[1])]\n",
    "labels  = labels.squeeze()\n",
    "\n",
    "from models import DSE\n",
    "model_DSE = DSE()\n",
    "nmi_list=[]\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "\n",
    "    pred = model_DSE.dse(X,7,2,500)\n",
    "    from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "    nmi = normalized_mutual_info_score(labels,pred)\n",
    "    # print(nmi)\n",
    "    nmi_list.append(nmi)\n",
    "print(np.mean(nmi_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
